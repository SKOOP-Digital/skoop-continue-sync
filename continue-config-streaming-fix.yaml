name: "Local Agent with LiteLLM Proxy"
version: "1.0.0"
schema: "v1"
description: "Continue agent using LiteLLM proxy for reasoning models"

# Models section - using the LiteLLM proxy with STREAMING enabled
models:
  # Claude Sonnet 4.5 via LiteLLM Proxy (with reasoning/thinking support)
  - name: "claude-sonnet-4-5"
    provider: ollama
    model: anthropic/claude-sonnet-4-5
    apiBase: http://localhost:8009
    roles:
      - chat
      - edit
    # IMPORTANT: Force streaming mode for Continue.dev compatibility
    streamOptions:
      enabled: true
    defaultCompletionOptions:
      contextLength: 200000
      maxTokens: 30000
      stream: true  # Force streaming
      reasoning: true
      reasoningBudgetTokens: 2048
      promptCaching: true  
    capabilities:
      - tool_use
      - image_input

rules:
  - name: "Code Quality"
    rule: "Write clean, readable, and maintainable code following best practices."

prompts:
  - name: "Code Review"
    description: "Review code for quality and best practices"
    prompt: "Please review this code for quality, security, and best practices."

context:
  - provider: file
  - provider: terminal
  - provider: problems
  - provider: tree
  - provider: code
  - provider: diff
  - provider: currentFile
  - provider: open
    params:
      onlyPinned: true
  - provider: debugger
    params:
      stackDepth: 3
  - provider: repo-map
  - provider: os
